# 算法岗专项面试题库

## 🎯 岗位定位
- 大模型算法工程师
- Agent 算法工程师
- RAG 算法工程师

## 🔍 核心考察点（基于真实JD）
1. **算法创新能力** - 如何改进现有算法？
2. **理论推导能力** - 能否手推公式、证明定理？
3. **实验设计能力** - 如何设计对比实验、消融实验？
4. **论文阅读能力** - 能否快速理解前沿论文？

---

## 第一部分：算法创新题（15题）

### RAG 检索优化

**Q1: 如何提升 GraphRAG 的多跳推理召回率？**
- 难度：⭐⭐⭐⭐
- 公司：字节、阿里（真题）
- 标签：#GraphRAG #多跳推理 #算法优化

---

**Q2: Agent Memory 压缩算法设计**
- 难度：⭐⭐⭐⭐
- 问题：长对话场景下，Memory 爆炸（10轮对话→5000 tokens）
- 目标：在保留关键信息的前提下，将 Memory 压缩至 30%
- 标签：#Memory #压缩算法 #Agent

---

**Q3: 设计一个自适应的 RAG 检索策略**
- 难度：⭐⭐⭐⭐
- 问题：不同查询需要不同检索深度，如何自适应调整？
- 标签：#RAG #自适应 #检索优化

---

**Q4: 如何优化 Agent 的规划效率？**
- 难度：⭐⭐⭐⭐
- 问题：ReAct 平均需要 8 步完成任务，如何减少到 5 步？
- 标签：#Agent #规划 #优化

---

**Q5: 多模态 RAG 的检索融合算法**
- 难度：⭐⭐⭐⭐⭐
- 问题：如何融合文本、图像、表格的检索结果？
- 标签：#多模态 #RAG #融合算法

---

### RLHF 与对齐优化

**Q6: 如何解决 Reward Hacking 问题？**
- 难度：⭐⭐⭐⭐
- 问题：奖励模型被钻空子，如何设计更鲁棒的奖励函数？
- 标签：#RLHF #RewardHacking #对齐

---

**Q7: DPO 与 PPO 的理论对比**
- 难度：⭐⭐⭐⭐
- 问题：从数学角度推导两者的关系，哪个更适合什么场景？
- 标签：#DPO #PPO #RLHF

---

**Q8: Token 级别奖励 vs Seq 级别奖励**
- 难度：⭐⭐⭐⭐⭐
- 问题：如何解决信用分配问题？设计 token 级奖励函数
- 标签：#RLHF #信用分配 #奖励设计

---

### Agent 协作与规划

**Q9: Multi-Agent 共识机制设计**
- 难度：⭐⭐⭐⭐⭐
- 问题：3 个 Agent 对同一问题给出不同答案，如何达成共识？
- 标签：#MultiAgent #共识机制 #协作

---

**Q10: Agent 自我修正算法优化**
- 难度：⭐⭐⭐⭐
- 问题：Reflexion 需要多轮反思，成本高，如何优化？
- 标签：#Agent #Reflection #优化

---

### 模型架构创新

**Q11: MoE 负载均衡算法改进**
- 难度：⭐⭐⭐⭐⭐
- 问题：某些专家过度使用，如何设计更好的路由策略？
- 标签：#MoE #负载均衡 #架构优化

---

**Q12: 长上下文建模的注意力机制优化**
- 难度：⭐⭐⭐⭐⭐
- 问题：如何在保持精度的前提下，将注意力复杂度从 O(n²) 降到 O(n log n)？
- 标签：#Attention #长上下文 #复杂度优化

---

**Q13: 小模型蒸馏策略设计**
- 难度：⭐⭐⭐⭐
- 问题：如何从 70B 模型蒸馏到 7B，保留 90% 能力？
- 标签：#蒸馏 #模型压缩 #优化

---

**Q14: Few-shot 学习的样本选择算法**
- 难度：⭐⭐⭐⭐
- 问题：给定 1000 个样本，如何选择最优的 10 个作为 Few-shot 示例？
- 标签：#FewShot #样本选择 #优化

---

**Q15: VLM 的跨模态对齐优化**
- 难度：⭐⭐⭐⭐⭐
- 问题：如何设计更高效的图文对齐损失函数？
- 标签：#VLM #跨模态 #对齐

---

## 第二部分：理论推导题（12题）

**Q1: 手推 PPO 损失函数，并解释 Clipped Objective 的作用**
- 难度：⭐⭐⭐⭐⭐
- 公司：字节（真题）
- 要求：白板推导完整公式
- 标签：#PPO #RLHF #数学推导

---

**Q2: 推导 Attention 机制的计算复杂度**
- 难度：⭐⭐⭐⭐
- 要求：分析时间复杂度和空间复杂度
- 延伸：Flash Attention 如何优化？
- 标签：#Attention #复杂度分析

---

**Q3: 推导 ROPE 的数学原理**
- 难度：⭐⭐⭐⭐⭐
- 要求：从旋转矩阵到最终公式的完整推导
- 标签：#ROPE #位置编码 #数学推导

---

**Q4: 推导 KL 散度在 RLHF 中的作用**
- 难度：⭐⭐⭐⭐
- 要求：解释为什么要约束 KL 散度，数学上如何实现
- 标签：#KL散度 #RLHF #理论

---

**Q5: 推导 LoRA 的参数量减少比例**
- 难度：⭐⭐⭐
- 要求：给定 rank=8，原始维度=4096，计算参数减少百分比
- 标签：#LoRA #参数计算 #优化

---

**Q6: 推导 Softmax 的梯度**
- 难度：⭐⭐⭐
- 要求：手推 Softmax 对输入的梯度公式
- 标签：#Softmax #梯度 #数学推导

---

**Q7: 推导 Cross-Entropy Loss 与 Negative Log-Likelihood 的关系**
- 难度：⭐⭐⭐
- 要求：证明两者等价
- 标签：#损失函数 #理论推导

---

**Q8: 推导 Layer Normalization 的前向和反向传播**
- 难度：⭐⭐⭐⭐
- 要求：完整推导公式
- 标签：#LayerNorm #前向传播 #反向传播

---

**Q9: 推导 Beam Search 的时间复杂度**
- 难度：⭐⭐⭐
- 要求：给定 beam_size=k，序列长度=n，词表大小=V，推导复杂度
- 标签：#BeamSearch #复杂度分析

---

**Q10: 推导 BLEU 分数的计算公式**
- 难度：⭐⭐
- 要求：从 n-gram 精确度到 BLEU-4 的完整推导
- 标签：#BLEU #评估指标 #公式推导

---

**Q11: 推导 Contrastive Loss（CLIP）的梯度**
- 难度：⭐⭐⭐⭐
- 要求：推导对比学习损失函数的梯度
- 标签：#ContrastiveLoss #CLIP #梯度推导

---

**Q12: 推导 Temperature Scaling 对概率分布的影响**
- 难度：⭐⭐⭐
- 要求：数学证明温度参数如何改变输出分布的熵
- 标签：#Temperature #采样 #理论

---

## 第三部分：实验设计题（10题）

**Q1: 如何设计 Agent 规划能力的评估实验？**
- 难度：⭐⭐⭐⭐
- 公司：阿里、腾讯（真题）
- 要求：数据集、Baseline、评估指标、消融实验
- 标签：#实验设计 #Agent #评估

---

**Q2: 设计 RAG 检索质量的对比实验**
- 难度：⭐⭐⭐⭐
- 要求：对比向量检索 vs BM25 vs 混合检索
- 标签：#RAG #实验设计 #对比实验

---

**Q3: 设计 RLHF 的消融实验**
- 难度：⭐⭐⭐⭐⭐
- 问题：如何证明奖励模型、PPO、KL 惩罚各自的贡献？
- 标签：#RLHF #消融实验 #实验设计

---

**Q4: 设计 Multi-Agent 协作效率实验**
- 难度：⭐⭐⭐⭐
- 要求：对比单 Agent vs 多 Agent 在复杂任务上的表现
- 标签：#MultiAgent #实验设计 #效率评估

---

**Q5: 设计 Memory 压缩算法的评估实验**
- 难度：⭐⭐⭐⭐
- 要求：如何量化压缩率与信息保留率的权衡？
- 标签：#Memory #压缩 #实验设计

---

**Q6: 设计 Prompt Engineering 的 A/B 测试**
- 难度：⭐⭐⭐
- 要求：对比不同 Prompt 策略的效果
- 标签：#Prompt #ABTest #实验设计

---

**Q7: 设计长上下文模型的压力测试**
- 难度：⭐⭐⭐⭐
- 要求：测试模型在不同上下文长度下的表现退化
- 标签：#长上下文 #压力测试 #实验

---

**Q8: 设计 VLM 幻觉问题的评估实验**
- 难度：⭐⭐⭐⭐
- 要求：如何构建数据集、设计指标、对比不同模型
- 标签：#VLM #幻觉 #实验设计

---

**Q9: 设计模型蒸馏效果的评估实验**
- 难度：⭐⭐⭐
- 要求：对比不同蒸馏策略（KD、Feature Matching、Response-based）
- 标签：#蒸馏 #实验设计 #对比实验

---

**Q10: 设计 Few-shot 学习的样本数量实验**
- 难度：⭐⭐⭐
- 要求：研究 0-shot, 1-shot, 5-shot, 10-shot 的性能曲线
- 标签：#FewShot #实验设计 #性能分析

---

## 第四部分：论文解读题（8题）

**Q1: DeepSeek-V3 的主要创新点是什么？**
- 难度：⭐⭐⭐
- 要求：读过论文原文
- 标签：#论文解读 #DeepSeek #MoE

---

**Q2: Qwen 系列的核心技术演进**
- 难度：⭐⭐⭐
- 要求：对比 Qwen 1.0 → 2.0 → 2.5 的改进
- 标签：#论文解读 #Qwen #技术演进

---

**Q3: o1 模型的推理能力来自哪里？**
- 难度：⭐⭐⭐⭐
- 要求：解释强化学习在推理任务中的作用
- 标签：#论文解读 #推理 #RL

---

**Q4: GraphRAG 论文的核心贡献**
- 难度：⭐⭐⭐
- 要求：解释图结构如何改进 RAG
- 标签：#论文解读 #GraphRAG

---

**Q5: Llama 3 的训练技巧**
- 难度：⭐⭐⭐
- 要求：解释数据配比、训练稳定性、长上下文扩展
- 标签：#论文解读 #Llama3 #训练

---

**Q6: Flash Attention 的优化原理**
- 难度：⭐⭐⭐⭐
- 要求：解释如何利用 GPU 内存层次结构优化
- 标签：#论文解读 #FlashAttention #优化

---

**Q7: DPO 论文的理论推导**
- 难度：⭐⭐⭐⭐⭐
- 要求：从 RLHF 到 DPO 的数学推导
- 标签：#论文解读 #DPO #理论

---

**Q8: 最近读过的 Agent 前沿论文**
- 难度：⭐⭐⭐
- 要求：自选一篇论文，讲清问题、方法、实验
- 标签：#论文解读 #Agent #前沿

---

## 🎯 高频算法岗真题（Top 10）

1. **如何优化 RAG 的检索召回率？**（必考）
2. **Agent Memory 设计方案？**（必考）
3. **手推 PPO/DPO 损失函数？**
4. **多智能体协作的共识机制设计？**
5. **如何评估 Agent 的规划能力？**
6. **RLHF 中的 Reward Hacking 如何解决？**
7. **GraphRAG vs Naive RAG 的理论分析？**
8. **如何设计 Agent 的自我修正机制？**
9. **最近读过哪些 Agent 方向的论文？**
10. **针对某个问题，如何设计消融实验？**
